{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65884a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pyreadr\n",
    "from scipy.linalg import toeplitz\n",
    "import doubleml as dml\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "import sklearn\n",
    "sklearn_version = sklearn.__version__\n",
    "\n",
    "\n",
    "def dgp_atte(n_obs=500, dim_x=20, theta=0, R2_d=0.5, R2_y=0.5,schwellwert=0):\n",
    "    \"\"\"\"\n",
    "    Generiert Daten aus (IRM) model.\n",
    "    The data generating process is inspired by a process used in the simulation experiment\"\"\"\n",
    "\n",
    "    v = np.random.uniform(size=[n_obs, ])\n",
    "    zeta = np.random.standard_normal(size=[n_obs, ])\n",
    "    cov_mat = toeplitz([np.power(0.5, k) for k in range(dim_x)])\n",
    "    x = np.random.multivariate_normal(np.zeros(dim_x), cov_mat, size=[n_obs, ])\n",
    "    beta = [1 / (k**2) for k in range(1, dim_x + 1)]\n",
    "    b_sigma_b = np.dot(np.dot(cov_mat, beta), beta)\n",
    "    c_y = np.sqrt(R2_y/((1-R2_y) * b_sigma_b))\n",
    "    c_d = np.sqrt(np.pi**2 / 3. * R2_d/((1-R2_d) * b_sigma_b))\n",
    "    xx = np.exp(np.dot(x, np.multiply(beta, c_d)))\n",
    "    d=np.zeros(n_obs)\n",
    "    d = 1. * ((xx/(1+xx)) > v)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #Um zu gewährleisten, dass viele Propensityscores nahe bei 0 oder 1 liegen wird hier ein % Wert festgelegt,\n",
    "    #der in etwa dem Minimum %-Wert an Beobachungen entspricht, der nahe bei 0 oder 1 liegen soll\n",
    "\n",
    "    # Berechnung des (Schwellwert/2) % Perzentils von X1, um \n",
    "    #d für alle X1 Werte, die in den top (schwellwert/2 %) \n",
    "    #sind auf 1 zu setzen\n",
    "    perzentil_X1 = np.percentile(x[:, 0], 100-schwellwert/2)\n",
    "    # Setzen von d auf 1, wenn X1 größer ist \n",
    "    #als das schwellwert/2. Perzentil\n",
    "    for i in range(n_obs):\n",
    "        if x[i, 0] > perzentil_X1:\n",
    "            d[i] = 1\n",
    "\n",
    "    # Berechnung des (Schwellwert/2) Perzentils von X2 \n",
    "    #analog zu X1 Manipulation nur, dass d auf 0 gesetzt \n",
    "    #wird wenn X2 klein ist\n",
    "    perzentil_X2 = np.percentile(x[:, 1], (schwellwert)/2)\n",
    "\n",
    "    for i in range(n_obs):\n",
    "        if x[i, 1] < perzentil_X2:\n",
    "            d[i] = 0\n",
    "        \n",
    "    #print(\"perzx2:\",perzentil_X2,\"min:\",min(x[:, 1]))\n",
    "\n",
    "    #Weiterführung der ursprünglichen Datengenerierung\n",
    "    y = d * theta + d * np.dot(x, np.multiply(beta, c_y)) + zeta\n",
    "    y0 = zeta\n",
    "    y1 = theta + np.dot(x, np.multiply(beta, c_y)) + zeta\n",
    "\n",
    "    #Berechnung von ATTE und ATE\n",
    "    ATTE = np.mean(y1[d == 1] - y0[d == 1])\n",
    "    ATE = np.mean(y1 - y0)\n",
    "    x_cols = [f'X{i + 1}' for i in np.arange(dim_x)]\n",
    "    data = pd.DataFrame(np.column_stack((x, y, d)),\n",
    "    columns=x_cols + ['y', 'd'])\n",
    "    \n",
    "    results = {'ATTE': ATTE,\n",
    "    'ATE': ATE,\n",
    "    'data': data}\n",
    "    return results\n",
    "\n",
    "\n",
    "if sklearn_version == '1.0.2':\n",
    "    penalty = 'none'\n",
    "else:\n",
    "    penalty = None\n",
    "    \n",
    "# Use linear and logistic regression in DoubleML\n",
    "ml_g = LinearRegression()\n",
    "ml_m = LogisticRegression(penalty=penalty)\n",
    "\n",
    "# Helper function to calculate whether CI covers true value\n",
    "def cover_true(theta, ki):\n",
    "    \"\"\"\n",
    "    Function to check whether theta is contained in confindence interval.\n",
    "    Returns 1 if true and 0 otherwise.\n",
    "    Input:\n",
    "    ------\n",
    "    theta: true value of target coefficent (ATE or ATTE ...)\n",
    "    confint: pd.series with lower and upper confidence bound\n",
    "    \"\"\"\n",
    "   \n",
    "    \n",
    "\n",
    "    if(ki.loc[\"d\",\"2.5 %\"] < theta and theta < ki.loc[\"d\",\"97.5 %\"]):\n",
    "        return  1\n",
    "    else:\n",
    "    \n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcfecb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "74450cc2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "100bfed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zusammenfassung: \n",
      " Anteil      0.117991\n",
      "ATE/ATTE    3.707617\n",
      "Dcoef       3.621245\n",
      "Tcoef       3.706148\n",
      "Dse         0.191325\n",
      "Tse         0.180189\n",
      "Dbias       0.086372\n",
      "Tbias       0.001469\n",
      "DaBias       0.17774\n",
      "TaBias      0.155519\n",
      "DqBias      0.053859\n",
      "TqBias      0.042119\n",
      "DsBias      0.959358\n",
      "TsBias      0.874314\n",
      "DinKI          0.904\n",
      "TinKI          0.936\n",
      "SKI_D       0.749982\n",
      "SKI_T       0.706328\n",
      "n_obs          800.0\n",
      "dtype: object\n",
      "rmse_discard: 0.23207571744467745 \n",
      " rmse_truncate: 0.20522977875634954\n"
     ]
    }
   ],
   "source": [
    "#Anzahl Simulationsdurchläufe\n",
    "n_sim = 1000\n",
    "#Anzahl der erzeugten Objekte je Simulationsdurchlauf\n",
    "n_obs = 800\n",
    "#Anzahl der zu erzeugenden Variablen exklusive D und Y\n",
    "dim_x = 10\n",
    "#Parameter Theta für den DGP festlegen\n",
    "theta = 3\n",
    "#Niveau des Konfidenzinterfalls bestimmen\n",
    "level = 0.95\n",
    "# Score art festlegen (ATTE oder ATE)\n",
    "score = \"ATTE\"\n",
    "#Schwellwert festlegen der ganz grob bestimmt, um wie stark der Anteil an ps-Werten nahe bei 0 oder 1  erhöht wird \n",
    "schwellwert = 50\n",
    "\n",
    "#Datenobjekte zur Speicherung der Daten Ergebnisse der einzelnen Simulationsdurchläufe anlegen\n",
    "sim_data = [None]*n_sim\n",
    "sim_ergebnisse = [None]*n_sim\n",
    "\n",
    "\n",
    "#Daten generieren und speichern für alle Durchläufe\n",
    "np.random.seed(431)\n",
    "\n",
    "for index_durchlauf in range(n_sim):\n",
    "    meineDaten = dgp_atte(theta = theta, n_obs = n_obs, dim_x = dim_x,schwellwert=schwellwert)\n",
    "    sim_data[index_durchlauf] = meineDaten\n",
    "\n",
    " \n",
    "    \n",
    "\n",
    "\n",
    "for index_durchlauf in range(n_sim):\n",
    "    data_dieserDurchlauf = sim_data[index_durchlauf]  \n",
    "\n",
    "    #Die eigentlichen Daten und den wahren ATE/ATTE extrahieren\n",
    "    data_extract=data_dieserDurchlauf['data']\n",
    "    truescore = data_dieserDurchlauf[score]\n",
    "    \n",
    "    #Daten in das DML Datenformat übergeben\n",
    "    data_dml = dml.DoubleMLData(data_extract, 'y', 'd')\n",
    "\n",
    "    #Für die 3 implementierten Varianten ein seperates dml_irm Objekt erzeugen mit den selben Daten, um Vergleichbarkeit zu gewährleisten\n",
    "    #dml_irm_none = dml.DoubleMLIRM(data_dml, ml_g, ml_m, score = score, trimming_rule=\"none\")\n",
    "    dml_irm_discard = dml.DoubleMLIRM(data_dml, ml_g, ml_m, score = score, trimming_rule=\"discard\")\n",
    "    dml_irm_truncate = dml.DoubleMLIRM(data_dml, ml_g, ml_m, score = score, trimming_rule=\"truncate\")\n",
    "\n",
    "    #Modelle aufstellen\n",
    "    #dml_irm_none.fit(store_predictions=True)\n",
    "    dml_irm_discard.fit(store_predictions=True)\n",
    "    dml_irm_truncate.fit(store_predictions=True)\n",
    "\n",
    "\n",
    "    #ps aus den predictions extrahieren und formatieren\n",
    "    #ps_none = pd.Series(dml_irm_none.predictions['ml_m'].reshape(dml_irm_none.predictions['ml_m'].size), name = 'ps_none')\n",
    "    ps_discard = pd.Series(dml_irm_discard.predictions['ml_m'].reshape(dml_irm_discard.predictions['ml_m'].size), name = 'ps_discard')\n",
    "    ps_truncate = pd.Series(dml_irm_truncate.predictions['ml_m'].reshape(dml_irm_truncate.predictions['ml_m'].size), name = 'ps_truncate')\n",
    "\n",
    "    # ps und ursprüngliche Daten in einem Objekt zusammenfassen als Vorbereitung für die Visualisierung\n",
    "    #newdata_none= pd.concat([data_extract,ps_none], axis=1)\n",
    "    newdata_discard= pd.concat([data_extract,ps_discard], axis=1)\n",
    "    newdata_truncate= pd.concat([data_extract,ps_truncate], axis=1)\n",
    "\n",
    "    #Koeffizienten speichern\n",
    "    #coef_none = dml_irm_none.coef\n",
    "    coef_discard = dml_irm_discard.coef\n",
    "    coef_truncate = dml_irm_truncate.coef\n",
    "\n",
    "    #Standartfehler speichern\n",
    "    #se_none = dml_irm_none.se\n",
    "    se_discard = dml_irm_discard.se\n",
    "    se_truncate = dml_irm_truncate.se\n",
    "\n",
    "    #Konfidenzintervalle speichern\n",
    "    #ki_none = dml_irm_none.confint(level = level)\n",
    "    ki_discard =  dml_irm_discard.confint(level = level)\n",
    "    ki_truncate =  dml_irm_truncate.confint(level = level)\n",
    "\n",
    "    #Spannweite der KI speichern\n",
    "    #ki_none_spannweite = ki_none.loc[\"d\",\"97.5 %\"] - ki_none.loc[\"d\",\"2.5 %\"]  \n",
    "    ki_discard_spannweite = ki_discard.loc[\"d\",\"97.5 %\"] - ki_discard.loc[\"d\",\"2.5 %\"]  \n",
    "    ki_truncate_spannweite = ki_truncate.loc[\"d\",\"97.5 %\"] -  ki_truncate.loc[\"d\",\"2.5 %\"]  \n",
    "\n",
    "  \n",
    "\n",
    "    #prüfen und speichern, ob das Konfidenzinterfall des Schätzers den wahren Wert umfasst\n",
    "    #kihit_none = cover_true(truescore,ki_none)\n",
    "    kihit_discard = cover_true(truescore,ki_discard)\n",
    "    kihit_truncate = cover_true(truescore,ki_truncate)\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "    #Bias berechnen\n",
    "    #Bias_none = truescore - coef_none\n",
    "    Bias_discard =  truescore - coef_discard \n",
    "    Bias_truncate= truescore - coef_truncate \n",
    "\n",
    "    #absoluten Bias berechnen\n",
    "    #aBias_none = np.abs(Bias_none)\n",
    "    aBias_discard =  np.abs(Bias_discard) \n",
    "    aBias_truncate= np.abs(Bias_truncate)\n",
    "\n",
    "    #quadrierten Bias berechnen\n",
    "    #qBias_none = np.square(Bias_none)\n",
    "    qBias_discard =  np.square(Bias_discard) \n",
    "    qBias_truncate= np.square(Bias_truncate)\n",
    "\n",
    "    #standartisierten Bias berechnen\n",
    "    #sBias_none = aBias_none / dml_irm_none.se\n",
    "    sBias_discard = aBias_discard / dml_irm_discard.se\n",
    "    sBias_truncate = aBias_truncate / dml_irm_truncate.se\n",
    "\n",
    "    # Berechnen des prozentualen Anteils der Beobachtungen, bei denen ps größer als 0,99 oder kleiner als 0,1 ist und somit nan\n",
    "    #\n",
    "    anteil = ps_discard.isna().mean()\n",
    "    \n",
    "    \n",
    "    #Deskriptive Statistiken erzeugen\n",
    "    #ds_none=ps_none.describe(percentiles=[.01, .05, .10, .25, .50, .75, .90, .95, .99]) \n",
    "    ds_discard=ps_discard.describe(percentiles=[.01, .05, .10, .25, .50, .75, .90, .95, .99]) \n",
    "    ds_truncate=ps_truncate.describe(percentiles=[.01, .05, .10, .25, .50, .75, .90, .95, .99]) \n",
    "    #print(\"ds_none:\",ds_none,type(ds_none))\n",
    "\n",
    "\n",
    "    ergebnisse = pd.DataFrame.from_dict({\"Anteil\":[anteil],\"ATE/ATTE\":[truescore],\n",
    "                                            \"Dcoef\":[coef_discard],\"Tcoef\":[coef_truncate],\n",
    "                                            \"Dse\":[se_discard],\"Tse\":[se_truncate],\n",
    "                                            \"Dbias\":[Bias_discard],\"Tbias\":[Bias_truncate],\n",
    "                                            \"DaBias\":[aBias_discard],  \"TaBias\":[aBias_truncate],\n",
    "                                            \"DqBias\":[qBias_discard], \"TqBias\":[qBias_truncate],\n",
    "                                            \"DsBias\":[sBias_discard],   \"TsBias\":[sBias_truncate],\n",
    "                                            \"DinKI\":[kihit_discard],\"TinKI\":[kihit_truncate],\n",
    "                                            \"SKI_D\":[ki_discard_spannweite],\"SKI_T\":[ki_truncate_spannweite],\n",
    "                                            \"n_obs\":[n_obs] })\n",
    "\n",
    "\n",
    "    \n",
    "    sim_ergebnisse[index_durchlauf] = ergebnisse\n",
    "    #\n",
    "\n",
    "    # \"Ncoef\":[coef_none], \"Nse\":[se_none], \"Nbias\":[Bias_none], \"NaBias\":[aBias_none], \"NqBias\":[qBias_none], \"NsBias\":[sBias_none], \"NinKI\":[kihit_none],\n",
    "    #\"SKI_N\":[ki_none_spannweite],\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    #CODE AB HIER AUSKOMMENTIERT LASSEN ODER ANZAHL SIMULATIONEN (n_sim)  AUF 1 SETZEN!!!!!!\n",
    "    #Diente den Voruntersuchungen und erzeugt für JEDEN Durchlauf 3 Histogramme und 3 Boxplots für die Verteilung des ps.\n",
    "    #Wurde an dieser Stelle bewusst behalten, weil praktisch, komfortabel und ganz spannend wenn man ein wenig rumspielen/untersuchen möchte\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "# Boxplot für Variante none    \n",
    "#plt.figure(figsize=(5,3))\n",
    "#plt.boxplot(ps_none,vert=False)\n",
    "#plt.title(dml_irm_none.trimming_rule)\n",
    "#plt.xlabel(\"ps\")\n",
    "#plt.show()\n",
    "\n",
    "# # #Der Boxplot kann mit den nan werten nicht umgehen, die beim discarden erzeugt werden:\n",
    "# ps_discard=ps_discard.dropna(axis=0)\n",
    "\n",
    "# # Boxplot für Variante discard\n",
    "# plt.figure(figsize=(5,3))\n",
    "# plt.boxplot(ps_discard,vert=False)\n",
    "# plt.title(dml_irm_discard.trimming_rule)\n",
    "# plt.xlabel(\"ps\")\n",
    "# plt.show()\n",
    "\n",
    "# # Boxplot für Variante truncate    \n",
    "# plt.figure(figsize=(5,3))\n",
    "# plt.boxplot(ps_truncate,vert=False)\n",
    "# plt.title(dml_irm_truncate.trimming_rule)\n",
    "# plt.xlabel(\"ps\")\n",
    "# plt.show()\n",
    "\n",
    "# #Aufteilen nach treatmentstatus, um die Verteilung des ps in Abhängigkeit vom Treatmentstatus darzustellen\n",
    "# #d1_data_none = newdata_none[newdata_none['d'] == 1]\n",
    "# #d0_data_none = newdata_none[newdata_none['d'] == 0]\n",
    "\n",
    "# d1_data_discard = newdata_discard[newdata_discard['d'] == 1]\n",
    "# d0_data_discard = newdata_discard[newdata_discard['d'] == 0]\n",
    "\n",
    "# d1_data_truncate = newdata_truncate[newdata_truncate['d'] == 1]\n",
    "# d0_data_truncate = newdata_truncate[newdata_truncate['d'] == 0]\n",
    "\n",
    "# #Erstellen des Histogramms für Verteilung des (ps | d) Variante none\n",
    "# #plt.hist(d1_data_none['ps_none'], bins=20, alpha=0.8, label='d = 1')\n",
    "# #plt.hist(d0_data_none['ps_none'], bins=20, alpha=0.5, label='d = 0')\n",
    "# #plt.xlabel('ps')\n",
    "# #plt.ylabel('Absolute Anzahl')\n",
    "# #plt.title(dml_irm_none.trimming_rule)\n",
    "# #plt.show()\n",
    "\n",
    "# #Erstellen des Histogramms für Verteilung des (ps | d)  Variante discard\n",
    "# plt.hist(d1_data_discard['ps_discard'], bins=20, alpha=0.8, label='d = 1')\n",
    "# plt.hist(d0_data_discard['ps_discard'], bins=20, alpha=0.5, label='d = 0')\n",
    "# plt.xlabel('ps')\n",
    "# plt.ylabel('Absolute Anzahl')\n",
    "# plt.title(dml_irm_discard.trimming_rule)\n",
    "# plt.show()\n",
    "\n",
    "# # #Erstellen des Histogramms  für Verteilung des (ps | d)  Variante truncate\n",
    "# plt.hist(d1_data_truncate['ps_truncate'], bins=20, alpha=0.8, label='d = 1')\n",
    "# plt.hist(d0_data_truncate['ps_truncate'], bins=20, alpha=0.5, label='d = 0')\n",
    "# plt.xlabel('ps')\n",
    "# plt.ylabel('Absolute Anzahl')\n",
    "# plt.title(dml_irm_truncate.trimming_rule)\n",
    "# plt.show()\n",
    "\n",
    "    \n",
    "#Ergebnisse speichern\n",
    "speicher = pd.concat(sim_ergebnisse)\n",
    "\n",
    "#Ergebnisse zusammenfassen und ausgeben\n",
    "zusammenfassung = speicher.mean(axis=0)\n",
    "print(\"zusammenfassung:\",\"\\n\",zusammenfassung)\n",
    "\n",
    "#RMSE berechnen:\n",
    "#rmse_none = np.sqrt(zusammenfassung[\"NqBias\"])\n",
    "rmse_discard = np.sqrt(zusammenfassung[\"DqBias\"])\n",
    "rmse_truncate = np.sqrt(zusammenfassung[\"TqBias\"])   \n",
    "print(\"rmse_discard:\",rmse_discard,\"\\n\",\"rmse_truncate:\",rmse_truncate)            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datenobjektgröße prä: (200, 13)\n",
      "maxPS: 0.9995713217488895\n",
      "minPS: 0.00040052568799330685\n",
      "datenobjektgröße post: (181, 13)\n",
      "maxPS: 0.989922806408379\n",
      "minPS: 0.011609179148799813\n",
      "\n",
      " \n",
      " Durchlauf 2: \n",
      "\n",
      "datenobjektgröße prä: (181, 13)\n",
      "maxPS: 0.9993532714406215\n",
      "minPS: 0.0034465159443917794\n",
      "datenobjektgröße post: (171, 13)\n",
      "maxPS: 0.9877666895515048\n",
      "minPS: 0.010947725253238992\n",
      "\n",
      " \n",
      " Durchlauf 3: \n",
      "\n",
      "datenobjektgröße prä: (171, 13)\n",
      "maxPS: 0.9987720232006024\n",
      "minPS: 0.008437245172404266\n",
      "datenobjektgröße post: (169, 13)\n",
      "maxPS: 0.988157713067804\n",
      "minPS: 0.012645028329871383\n",
      "\n",
      " \n",
      " Durchlauf 4: \n",
      "\n",
      "datenobjektgröße prä: (169, 13)\n",
      "maxPS: 0.9893909734021283\n",
      "minPS: 0.010258503312758205\n",
      "datenobjektgröße post: (169, 13)\n",
      "maxPS: 0.9893909734021283\n",
      "minPS: 0.010258503312758205\n",
      "\n",
      " \n",
      " Durchlauf 5: \n",
      "\n",
      "datenobjektgröße prä: (169, 13)\n",
      "maxPS: 0.9911485671038283\n",
      "minPS: 0.005894849475390491\n",
      "datenobjektgröße post: (165, 13)\n",
      "maxPS: 0.988464333415763\n",
      "minPS: 0.013405840997927744\n",
      "\n",
      " \n",
      " Durchlauf 6: \n",
      "\n",
      "datenobjektgröße prä: (165, 13)\n",
      "maxPS: 0.996472833822684\n",
      "minPS: 0.003412748587693031\n",
      "datenobjektgröße post: (156, 13)\n",
      "maxPS: 0.987847807088878\n",
      "minPS: 0.01331808213707818\n",
      "\n",
      " \n",
      " Durchlauf 7: \n",
      "\n",
      "datenobjektgröße prä: (156, 13)\n",
      "maxPS: 0.9986038868552974\n",
      "minPS: 0.0018969523348498687\n",
      "datenobjektgröße post: (147, 13)\n",
      "maxPS: 0.9883056152372721\n",
      "minPS: 0.010178625274920165\n",
      "\n",
      " \n",
      " Durchlauf 8: \n",
      "\n",
      "datenobjektgröße prä: (147, 13)\n",
      "maxPS: 0.9944868760294098\n",
      "minPS: 0.008941775841115343\n",
      "datenobjektgröße post: (142, 13)\n",
      "maxPS: 0.9882732709922692\n",
      "minPS: 0.010425939350677543\n",
      "\n",
      " \n",
      " Durchlauf 9: \n",
      "\n",
      "datenobjektgröße prä: (142, 13)\n",
      "maxPS: 0.9927225248746935\n",
      "minPS: 0.004011741609213214\n",
      "datenobjektgröße post: (137, 13)\n",
      "maxPS: 0.9876166045579292\n",
      "minPS: 0.014392951268646766\n"
     ]
    }
   ],
   "source": [
    "def discard_naive(dataparameter,rule):\n",
    "\n",
    "    #Übergabe der Daten und Aufstellen des Modells\n",
    "    data_dml = dml.DoubleMLData(dataparameter,\n",
    "                                 'y', 'd')\n",
    "    dml_irm = dml.DoubleMLIRM(data_dml, ml_g, ml_m,\n",
    "                               score = 'ATTE',\n",
    "                                 trimming_rule=rule)\n",
    "    dml_irm.fit()\n",
    "\n",
    "    #ps aus den predictions extrahieren und \n",
    "    #formatieren, um an newdata anzuhängen\n",
    "    ps = pd.Series(dml_irm.predictions['ml_m'].\n",
    "                   reshape(dml_irm.predictions['ml_m'].\n",
    "                           size), name = 'ps')\n",
    "    newdata= pd.concat([dataparameter,ps], axis=1)\n",
    "\n",
    "    print('datenobjektgröße prä:', newdata.shape)\n",
    "    print('maxPS:' , max(newdata['ps']))\n",
    "    print('minPS:' , min(newdata['ps']))\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    #Aufteilen nach treatmentstatus, um ursprüngliche Daten darzustellen\n",
    "    d1_data = newdata[newdata['d'] == 1]\n",
    "    d0_data = newdata[newdata['d'] == 0]\n",
    "\n",
    "    #2 Erstellen des Histogramms aller Daten für Verteilung des ps\n",
    "    # plt.hist(d1_data['ps'], bins=20, alpha=0.8, label='d = 1')\n",
    "    # plt.hist(d0_data['ps'], bins=20, alpha=0.5, label='d = 0')\n",
    "    # plt.xlabel('ps')\n",
    "    # plt.ylabel('Absolute Anzahl')\n",
    "    # plt.legend('a')\n",
    "    # plt.show()\n",
    "\n",
    "    #Observierungen mit ps kleiner trimmingthreshold\n",
    "    #oder  größer 1-trimmmingthreshold löschen\n",
    "  \n",
    "    \n",
    "    newdata_droped=newdata.drop(newdata[(newdata['ps']<=dml_irm.trimming_threshold)|(newdata['ps']>=(1-dml_irm.trimming_threshold))].index)\n",
    "    newdata_droped=newdata_droped.reset_index(drop=True)\n",
    "    #print('newdata_droped:' ,type(newdata_droped), newdata_droped.shape, newdata_droped.size)\n",
    "    #print('maxd:' , max(newdata_droped['ps']))\n",
    "    #print('mind:' , min(newdata_droped['ps']))\n",
    "\n",
    "    #erneut aufteilen um nun ps verteilung des bereinigten Datensatzes darstellen\n",
    "    d1_data_droped = newdata_droped[newdata_droped['d'] == 1]\n",
    "    d0_data_droped = newdata_droped[newdata_droped['d'] == 0]\n",
    "\n",
    "    # Erstellen des Histogramms für bereinigte Daten\n",
    "    # plt.hist(d1_data_droped['ps'], bins=20, alpha=0.8, label='d = 1')\n",
    "    # plt.hist(d0_data_droped['ps'], bins=20, alpha=0.5, label='d = 0')\n",
    "    # plt.xlabel('ps')\n",
    "    # plt.ylabel('Absolute Anzahl')\n",
    "    # plt.legend('b')\n",
    "    # plt.show()\n",
    "\n",
    "    print('datenobjektgröße post:' ,newdata_droped.shape)\n",
    "    print('maxPS:' , max(newdata_droped['ps']))\n",
    "    print('minPS:' , min(newdata_droped['ps']))\n",
    "    \n",
    "    #ps wird nun wieder entfernt, da nur mit den \n",
    "    #bereinigten Daten der gesamte Vorgang \n",
    "    #wiederholt werden soll.\n",
    "    newdata = newdata_droped.drop('ps', axis=1)\n",
    "\n",
    "    \n",
    "    \n",
    "    return newdata\n",
    "\n",
    "\n",
    "\n",
    "meineDaten = dgp_atte(theta = 3,\n",
    "                       n_obs = 200,\n",
    "                         dim_x = 10,\n",
    "                         schwellwert=0)\n",
    "nurDaten=meineDaten['data']\n",
    "\n",
    "\n",
    "np.random.seed(4321)\n",
    "\n",
    "a = discard_naive(dataparameter=nurDaten,\n",
    "                  rule = \"none\")\n",
    "\n",
    "\n",
    "print(\"\\n\",\"\\n\",'Durchlauf 2:', \"\\n\")\n",
    "b = discard_naive(a,rule=\"none\")\n",
    "\n",
    "print(\"\\n\",\"\\n\",'Durchlauf 3:', '\\n')\n",
    "c = discard_naive(b,rule=\"none\")\n",
    "\n",
    "print(\"\\n\",\"\\n\",'Durchlauf 4:', '\\n')\n",
    "d = discard_naive(c,rule=\"none\")\n",
    "\n",
    "print(\"\\n\",\"\\n\",'Durchlauf 5:', '\\n')\n",
    "e = discard_naive(d,rule=\"none\")\n",
    "\n",
    "print(\"\\n\",\"\\n\",'Durchlauf 6:', \"\\n\")\n",
    "f = discard_naive(e,rule=\"none\")\n",
    "\n",
    "print(\"\\n\",\"\\n\",'Durchlauf 7:', '\\n')\n",
    "g = discard_naive(f,rule=\"none\")\n",
    "\n",
    "print(\"\\n\",\"\\n\",'Durchlauf 8:', '\\n')\n",
    "h = discard_naive(g,rule=\"none\")\n",
    "\n",
    "print(\"\\n\",\"\\n\",'Durchlauf 9:', '\\n')\n",
    "i = discard_naive(h,rule=\"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f66d64b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
